{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded3c13-027b-4a4d-a6a8-555bcda47cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "# Data loading\n",
    "df = pd.read_csv('your_data.csv')\n",
    "\n",
    "# Example feature engineering (customize as per your data)\n",
    "X = df.drop('target', axis=1)\n",
    "y = df['target']\n",
    "\n",
    "# Simple preprocessing\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model training\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions and evaluations\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "print(\"Classification report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"ROC AUC score:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(\"Average Precision (PR AUC):\", average_precision_score(y_test, y_pred_proba))\n",
    "\n",
    "# SHAP analysis\n",
    "explainer_shap = shap.Explainer(model, X_train)\n",
    "shap_values = explainer_shap(X_test)\n",
    "shap.summary_plot(shap_values, X_test, feature_names=X.columns)\n",
    "\n",
    "# Top 10 SHAP features\n",
    "mean_shap = np.abs(shap_values.values).mean(axis=0)\n",
    "top_features = np.argsort(mean_shap)[-10:][::-1]\n",
    "print(\"Top 10 SHAP features:\")\n",
    "for i in top_features:\n",
    "    print(f\"{X.columns[i]}: {mean_shap[i]:.4f}\")\n",
    "\n",
    "# LIME analysis for a single test instance\n",
    "explainer_lime = lime.lime_tabular.LimeTabularExplainer(\n",
    "    X_train, feature_names=X.columns, class_names=['0', '1'], discretize_continuous=True, mode='classification')\n",
    "i = 0  # Index of the instance you want to explain\n",
    "exp = explainer_lime.explain_instance(X_test[i], model.predict_proba, num_features=10)\n",
    "print(\"LIME explanation for first test instance:\")\n",
    "print(exp.as_list())\n",
    "\n",
    "# Comparative text-based summary\n",
    "print(\"\"\"\n",
    "Text-based Comparative Analysis:\n",
    "Both SHAP and LIME offer insights into feature importances for local predictions.\n",
    "SHAP provides additive feature attributions based on game theory, ensuring consistency and local accuracy.\n",
    "LIME approximates the model locally using an interpretable linear model.\n",
    "In this analysis, SHAP's top features were: {}\n",
    "LIME's top features (for instance 0) were: {}\n",
    "\"\"\".format(\n",
    "    ', '.join([X.columns[i] for i in top_features]),\n",
    "    ', '.join([f\"{x[0]} ({x[1]:.4f})\" for x in exp.as_list()])\n",
    "))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:anaconda-2025.06-py3.11]",
   "language": "python",
   "name": "conda-env-anaconda-2025.06-py3.11-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
